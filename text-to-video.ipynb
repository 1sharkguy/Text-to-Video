{"metadata":{"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing Libraries","metadata":{"id":"GFFrVzR9j8s7"}},{"cell_type":"code","source":"# Operating System module for interacting with the operating system\nimport os\n\n# Module for generating random numbers\nimport random\n\n# Module for numerical operations\nimport numpy as np\n\n# OpenCV library for image processing\nimport cv2\n\n# Python Imaging Library for image processing\nfrom PIL import Image, ImageDraw, ImageFont\n\n# PyTorch library for deep learning\nimport torch\n\n# Dataset class for creating custom datasets in PyTorch\nfrom torch.utils.data import Dataset\n\n# Module for image transformations\nimport torchvision.transforms as transforms\n\n# Neural network module in PyTorch\nimport torch.nn as nn\n\n# Optimization algorithms in PyTorch\nimport torch.optim as optim\n\n# Function for padding sequences in PyTorch\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Function for saving images in PyTorch\nfrom torchvision.utils import save_image\n\n# Module for plotting graphs and images\nimport matplotlib.pyplot as plt\n\n# Module for displaying rich content in IPython environments\nfrom IPython.display import clear_output, display, HTML\n\n# Module for encoding and decoding binary data to text\nimport base64","metadata":{"id":"_vABkDvGj8s7","execution":{"iopub.status.busy":"2024-10-16T03:47:48.144739Z","iopub.execute_input":"2024-10-16T03:47:48.145661Z","iopub.status.idle":"2024-10-16T03:47:51.739663Z","shell.execute_reply.started":"2024-10-16T03:47:48.145616Z","shell.execute_reply":"2024-10-16T03:47:51.738677Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Coding the Training Data","metadata":{"id":"Mg5sx3Qnj8s8"}},{"cell_type":"code","source":"# Define the number of videos to generate for the dataset\nnum_videos = 30000\n\n# Define the number of frames per video (1 Second Video)\nframes_per_video = 10\n\n# Define the size of each image in the dataset\nimg_size = (64, 64)\n\n# Define the size of the shapes (Circle)\nshape_size = 10\n\n# Define text prompts and corresponding movements for circles\nprompts_and_movements = [\n    (\"circle moving down\", \"circle\", \"down\"),  # Move circle downward\n    (\"circle moving left\", \"circle\", \"left\"),  # Move circle leftward\n    (\"circle moving right\", \"circle\", \"right\"),  # Move circle rightward\n    (\"circle moving diagonally up-right\", \"circle\", \"diagonal_up_right\"),  # Move circle diagonally up-right\n    (\"circle moving diagonally down-left\", \"circle\", \"diagonal_down_left\"),  # Move circle diagonally down-left\n    (\"circle moving diagonally up-left\", \"circle\", \"diagonal_up_left\"),  # Move circle diagonally up-left\n    (\"circle moving diagonally down-right\", \"circle\", \"diagonal_down_right\"),  # Move circle diagonally down-right\n    (\"circle rotating clockwise\", \"circle\", \"rotate_clockwise\"),  # Rotate circle clockwise\n    (\"circle rotating counter-clockwise\", \"circle\", \"rotate_counter_clockwise\"),  # Rotate circle counter-clockwise\n    (\"circle bouncing vertically\", \"circle\", \"bounce_vertical\"),  # Bounce circle vertically\n    (\"circle bouncing horizontally\", \"circle\", \"bounce_horizontal\"),  # Bounce circle horizontally\n    (\"circle zigzagging vertically\", \"circle\", \"zigzag_vertical\"),  # Zigzag circle vertically\n    (\"circle zigzagging horizontally\", \"circle\", \"zigzag_horizontal\"),  # Zigzag circle horizontally\n    (\"circle moving up-left\", \"circle\", \"up_left\"),  # Move circle up-left\n    (\"circle moving down-right\", \"circle\", \"down_right\"),  # Move circle down-right\n    (\"circle moving down-left\", \"circle\", \"down_left\")  # Move circle down-left\n]\n\n# Define a function to create an image with a moving shape\ndef create_image_with_moving_shape(size, frame_num, shape, direction):\n    # Create a new RGB image with the specified size and white background\n    img = Image.new('RGB', size, color=(255, 255, 255))\n    draw = ImageDraw.Draw(img)\n\n    # Calculate the initial position of the shape (center of the image)\n    center_x, center_y = size[0] // 2, size[1] // 2\n\n    # Determine the shape position based on the movement direction\n    if direction == \"down\":\n        position = (center_x, (center_y + frame_num * 5) % size[1])\n    elif direction == \"left\":\n        position = ((center_x - frame_num * 5) % size[0], center_y)\n    elif direction == \"right\":\n        position = ((center_x + frame_num * 5) % size[0], center_y)\n    elif direction == \"diagonal_up_right\":\n        position = ((center_x + frame_num * 5) % size[0], (center_y - frame_num * 5) % size[1])\n    elif direction == \"diagonal_down_left\":\n        position = ((center_x - frame_num * 5) % size[0], (center_y + frame_num * 5) % size[1])\n    elif direction == \"diagonal_up_left\":\n        position = ((center_x - frame_num * 5) % size[0], (center_y - frame_num * 5) % size[1])\n    elif direction == \"diagonal_down_right\":\n        position = ((center_x + frame_num * 5) % size[0], (center_y + frame_num * 5) % size[1])\n    elif direction == \"rotate_clockwise\":\n        img = img.rotate(frame_num * 10, center=(center_x, center_y), fillcolor=(255, 255, 255))\n        position = (center_x, center_y)\n    elif direction == \"rotate_counter_clockwise\":\n        img = img.rotate(-frame_num * 10, center=(center_x, center_y), fillcolor=(255, 255, 255))\n        position = (center_x, center_y)\n    elif direction == \"bounce_vertical\":\n        position = (center_x, center_y - abs(frame_num * 5 % size[1] - center_y))\n    elif direction == \"bounce_horizontal\":\n        position = (center_x - abs(frame_num * 5 % size[0] - center_x), center_y)\n    elif direction == \"zigzag_vertical\":\n        position = (center_x, center_y - frame_num * 5 % size[1] if frame_num % 2 == 0 else center_y + frame_num * 5 % size[1])\n    elif direction == \"zigzag_horizontal\":\n        position = (center_x - frame_num * 5 % size[0] if frame_num % 2 == 0 else center_x + frame_num * 5 % size[0], center_y)\n    elif direction == \"up_left\":\n        position = ((center_x - frame_num * 5) % size[0], (center_y - frame_num * 5) % size[1])\n    elif direction == \"down_right\":\n        position = ((center_x + frame_num * 5) % size[0], (center_y + frame_num * 5) % size[1])\n    elif direction == \"down_left\":\n        position = ((center_x - frame_num * 5) % size[0], (center_y + frame_num * 5) % size[1])\n    else:\n        position = (center_x, center_y)\n\n    # Draw the shape (circle) at the calculated position\n    if shape == \"circle\":\n        draw.ellipse([position[0] - shape_size // 2, position[1] - shape_size // 2, position[0] + shape_size // 2, position[1] + shape_size // 2], fill=(0, 0, 255))\n\n    # Return the image as a numpy array\n    return np.array(img)\n\n# Generate the dataset\nfor video_num in range(num_videos):\n    prompt, shape, direction = random.choice(prompts_and_movements)\n    video_frames = []\n    for frame_num in range(frames_per_video):\n        img_array = create_image_with_moving_shape(img_size, frame_num, shape, direction)\n        video_frames.append(img_array)\n\n    # Save the frames as images in the training dataset directory\n    video_dir = os.path.join('/kaggle/working/training_dataset/', f'video_{video_num}')\n    os.makedirs(video_dir, exist_ok=True)\n    for frame_num, frame in enumerate(video_frames):\n        frame_image = Image.fromarray(frame)\n        frame_image.save(os.path.join(video_dir, f'frame_{frame_num}.png'))\n\nprint(\"Dataset generation complete.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Lh9GLYXj8s8","outputId":"0285032c-b1cd-4ab7-ba35-311484b27b52","execution":{"iopub.status.busy":"2024-10-16T03:47:51.741677Z","iopub.execute_input":"2024-10-16T03:47:51.742216Z","iopub.status.idle":"2024-10-16T03:49:51.289509Z","shell.execute_reply.started":"2024-10-16T03:47:51.742171Z","shell.execute_reply":"2024-10-16T03:49:51.288477Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Dataset generation complete.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Iterate over the number of videos to generate\nfor i in range(num_videos):\n    # Randomly choose a prompt and movement from the predefined list\n    prompt, shape, direction = random.choice(prompts_and_movements)\n\n    # Create a directory for the current video\n    video_dir = f'/kaggle/working/training_dataset/video_{i}'\n    os.makedirs(video_dir, exist_ok=True)\n\n    # Write the chosen prompt to a text file in the video directory\n    with open(f'{video_dir}/prompt.txt', 'w') as f:\n        f.write(prompt)\n\n    # Generate frames for the current video\n    for frame_num in range(frames_per_video):\n        # Create an image with a moving shape based on the current frame number, shape, and direction\n        img = create_image_with_moving_shape(img_size, frame_num, shape, direction)\n\n        # Save the generated image as a PNG file in the video directory\n        cv2.imwrite(f'{video_dir}/frame_{frame_num}.png', img)","metadata":{"id":"IF10vxGej8s8","execution":{"iopub.status.busy":"2024-10-16T03:49:51.290796Z","iopub.execute_input":"2024-10-16T03:49:51.291206Z","iopub.status.idle":"2024-10-16T03:50:54.836042Z","shell.execute_reply.started":"2024-10-16T03:49:51.291160Z","shell.execute_reply":"2024-10-16T03:50:54.834974Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Pre-Processing Our Training Data","metadata":{"id":"HXLQf-Wkj8s9"}},{"cell_type":"code","source":"# Define a dataset class inheriting from torch.utils.data.Dataset\nclass TextToVideoDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        # Initialize the dataset with root directory and optional transform\n        self.root_dir = root_dir\n        self.transform = transform\n        # List all subdirectories in the root directory\n        self.video_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n        # Initialize lists to store frame paths and corresponding prompts\n        self.frame_paths = []\n        self.prompts = []\n\n        # Loop through each video directory\n        for video_dir in self.video_dirs:\n            # List all PNG files in the video directory and store their paths\n            frames = [os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith('.png')]\n            self.frame_paths.extend(frames)\n            # Read the prompt text file in the video directory and store its content\n            with open(os.path.join(video_dir, 'prompt.txt'), 'r') as f:\n                prompt = f.read().strip()\n            # Repeat the prompt for each frame in the video and store in prompts list\n            self.prompts.extend([prompt] * len(frames))\n\n    # Return the total number of samples in the dataset\n    def __len__(self):\n        return len(self.frame_paths)\n\n    # Retrieve a sample from the dataset given an index\n    def __getitem__(self, idx):\n        # Get the path of the frame corresponding to the given index\n        frame_path = self.frame_paths[idx]\n        # Open the image using PIL (Python Imaging Library)\n        image = Image.open(frame_path)\n        # Get the prompt corresponding to the given index\n        prompt = self.prompts[idx]\n\n        # Apply transformation if specified\n        if self.transform:\n            image = self.transform(image)\n\n        # Return the transformed image and the prompt\n        return image, prompt\n\n# Define a set of transformations to be applied to the data\ntransform = transforms.Compose([\n    transforms.ToTensor(), # Convert PIL Image or numpy.ndarray to tensor\n    transforms.Normalize((0.5,), (0.5,)) # Normalize image with mean and standard deviation\n])\n\n# Load the dataset using the defined transform\ndataset = TextToVideoDataset(root_dir='/kaggle/working/training_dataset', transform=transform)\n# Create a dataloader to iterate over the dataset\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)","metadata":{"id":"JlSlYNKpj8s9","execution":{"iopub.status.busy":"2024-10-16T03:50:54.838226Z","iopub.execute_input":"2024-10-16T03:50:54.838547Z","iopub.status.idle":"2024-10-16T03:50:58.492968Z","shell.execute_reply.started":"2024-10-16T03:50:54.838513Z","shell.execute_reply":"2024-10-16T03:50:58.492139Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Implementing GAN Architecture","metadata":{"id":"OvaajlG2j8s9"}},{"cell_type":"code","source":"# Define a class for text embedding\nclass TextEmbedding(nn.Module):\n    # Constructor method with vocab_size and embed_size parameters\n    def __init__(self, vocab_size, embed_size):\n        # Call the superclass constructor\n        super(TextEmbedding, self).__init__()\n        # Initialize embedding layer\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n\n    # Define the forward pass method\n    def forward(self, x):\n        # Return embedded representation of input\n        return self.embedding(x)\n\nclass Generator(nn.Module):\n    def __init__(self, text_embed_size):\n        super(Generator, self).__init__()\n\n        # Fully connected layer that takes noise and text embedding as input\n        self.fc1 = nn.Linear(100 + text_embed_size, 256 * 8 * 8)\n\n        # Transposed convolutional layers to upsample the input\n        self.deconv1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n        self.deconv2 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n        self.deconv3 = nn.ConvTranspose2d(64, 3, 4, 2, 1)  # Output has 3 channels for RGB images\n\n        # Activation functions\n        self.relu = nn.ReLU(True)  # ReLU activation function\n        self.tanh = nn.Tanh()       # Tanh activation function for final output\n\n    def forward(self, noise, text_embed):\n        # Concatenate noise and text embedding along the channel dimension\n        x = torch.cat((noise, text_embed), dim=1)\n\n        # Fully connected layer followed by reshaping to 4D tensor\n        x = self.fc1(x).view(-1, 256, 8, 8)\n\n        # Upsampling through transposed convolution layers with ReLU activation\n        x = self.relu(self.deconv1(x))\n        x = self.relu(self.deconv2(x))\n\n        # Final layer with Tanh activation to ensure output values are between -1 and 1 (for images)\n        x = self.tanh(self.deconv3(x))\n\n        return x\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        # Convolutional layers to process input images\n        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)   # 3 input channels (RGB), 64 output channels, kernel size 4x4, stride 2, padding 1\n        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1) # 64 input channels, 128 output channels, kernel size 4x4, stride 2, padding 1\n        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1) # 128 input channels, 256 output channels, kernel size 4x4, stride 2, padding 1\n\n        # Fully connected layer for classification\n        self.fc1 = nn.Linear(256 * 8 * 8, 1)  # Input size 256x8x8 (output size of last convolution), output size 1 (binary classification)\n\n        # Activation functions\n        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)  # Leaky ReLU activation with negative slope 0.2\n        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for final output (probability)\n\n    def forward(self, input):\n        # Pass input through convolutional layers with LeakyReLU activation\n        x = self.leaky_relu(self.conv1(input))\n        x = self.leaky_relu(self.conv2(x))\n        x = self.leaky_relu(self.conv3(x))\n\n        # Flatten the output of convolutional layers\n        x = x.view(-1, 256 * 8 * 8)\n\n        # Pass through fully connected layer with Sigmoid activation for binary classification\n        x = self.sigmoid(self.fc1(x))\n\n        return x","metadata":{"id":"QKgbkSNaj8s9","execution":{"iopub.status.busy":"2024-10-16T03:50:58.494222Z","iopub.execute_input":"2024-10-16T03:50:58.494628Z","iopub.status.idle":"2024-10-16T03:50:58.517646Z","shell.execute_reply.started":"2024-10-16T03:50:58.494563Z","shell.execute_reply":"2024-10-16T03:50:58.516800Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Coding Training Parameters","metadata":{"id":"GfCQkirWj8s9"}},{"cell_type":"code","source":"# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Create a simple vocabulary for text prompts\nall_prompts = [prompt for prompt, _, _ in prompts_and_movements]  # Extract all prompts from prompts_and_movements list\nvocab = {word: idx for idx, word in enumerate(set(\" \".join(all_prompts).split()))}  # Create a vocabulary dictionary where each unique word is assigned an index\nvocab_size = len(vocab)  # Size of the vocabulary\nembed_size = 10  # Size of the text embedding vector\n\ndef encode_text(prompt):\n    # Encode a given prompt into a tensor of indices using the vocabulary\n    return torch.tensor([vocab[word] for word in prompt.split()])\n\n# Initialize models, loss function, and optimizers\ntext_embedding = TextEmbedding(vocab_size, embed_size).to(device)  # Initialize TextEmbedding model with vocab_size and embed_size\nnetG = Generator(embed_size).to(device)  # Initialize Generator model with embed_size\nnetD = Discriminator().to(device)  # Initialize Discriminator model\ncriterion = nn.BCELoss().to(device)  # Binary Cross Entropy loss function\noptimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Adam optimizer for Discriminator\noptimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Adam optimizer for Generator","metadata":{"id":"L9Zrz1szj8s9","execution":{"iopub.status.busy":"2024-10-16T03:50:58.518902Z","iopub.execute_input":"2024-10-16T03:50:58.519232Z","iopub.status.idle":"2024-10-16T03:50:58.842375Z","shell.execute_reply.started":"2024-10-16T03:50:58.519198Z","shell.execute_reply":"2024-10-16T03:50:58.841355Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Training Loop","metadata":{"id":"X2tChYiEj8s9"}},{"cell_type":"code","source":"# Number of epochs\nnum_epochs = 13\n\n# Iterate over each epoch\nfor epoch in range(num_epochs):\n    # Iterate over each batch of data\n    for i, (data, prompts) in enumerate(dataloader):\n        # Move real data to device\n        real_data = data.to(device)\n\n        # Convert prompts to list\n        prompts = [prompt for prompt in prompts]\n\n        # Update Discriminator\n        netD.zero_grad()  # Zero the gradients of the Discriminator\n        batch_size = real_data.size(0)  # Get the batch size\n        labels = torch.ones(batch_size, 1).to(device)  # Create labels for real data (ones)\n        output = netD(real_data)  # Forward pass real data through Discriminator\n        lossD_real = criterion(output, labels)  # Calculate loss on real data\n        lossD_real.backward()  # Backward pass to calculate gradients\n\n        # Generate fake data\n        noise = torch.randn(batch_size, 100).to(device)  # Generate random noise\n        text_embeds = torch.stack([text_embedding(encode_text(prompt).to(device)).mean(dim=0) for prompt in prompts])  # Encode prompts into text embeddings\n        fake_data = netG(noise, text_embeds)  # Generate fake data from noise and text embeddings\n        labels = torch.zeros(batch_size, 1).to(device)  # Create labels for fake data (zeros)\n        output = netD(fake_data.detach())  # Forward pass fake data through Discriminator (detach to avoid gradients flowing back to Generator)\n        lossD_fake = criterion(output, labels)  # Calculate loss on fake data\n        lossD_fake.backward()  # Backward pass to calculate gradients\n        optimizerD.step()  # Update Discriminator parameters\n\n        # Update Generator\n        netG.zero_grad()  # Zero the gradients of the Generator\n        labels = torch.ones(batch_size, 1).to(device)  # Create labels for fake data (ones) to fool Discriminator\n        output = netD(fake_data)  # Forward pass fake data (now updated) through Discriminator\n        lossG = criterion(output, labels)  # Calculate loss for Generator based on Discriminator's response\n        lossG.backward()  # Backward pass to calculate gradients\n        optimizerG.step()  # Update Generator parameters\n\n    # Print epoch information\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss D: {lossD_real + lossD_fake}, Loss G: {lossG}\")","metadata":{"id":"mlSboIs5j8s9","execution":{"iopub.status.busy":"2024-10-16T03:50:58.843569Z","iopub.execute_input":"2024-10-16T03:50:58.843906Z","iopub.status.idle":"2024-10-16T05:08:07.001461Z","shell.execute_reply.started":"2024-10-16T03:50:58.843856Z","shell.execute_reply":"2024-10-16T05:08:07.000457Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch [1/13] Loss D: 67.83155822753906, Loss G: 72.84817504882812\nEpoch [2/13] Loss D: 67.77769470214844, Loss G: 72.11215209960938\nEpoch [3/13] Loss D: 67.59626770019531, Loss G: 71.58956909179688\nEpoch [4/13] Loss D: 67.80558776855469, Loss G: 72.39151000976562\nEpoch [5/13] Loss D: 67.77468872070312, Loss G: 71.05729675292969\nEpoch [6/13] Loss D: 67.74132537841797, Loss G: 71.36183166503906\nEpoch [7/13] Loss D: 67.73070526123047, Loss G: 69.02749633789062\nEpoch [8/13] Loss D: 0.0, Loss G: 100.0\nEpoch [9/13] Loss D: 0.0, Loss G: 100.0\nEpoch [10/13] Loss D: 0.0, Loss G: 100.0\nEpoch [11/13] Loss D: 0.0, Loss G: 100.0\nEpoch [12/13] Loss D: 0.0, Loss G: 100.0\nEpoch [13/13] Loss D: 0.0, Loss G: 100.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Saving the Trained Model","metadata":{"id":"veLNs63Xj8s-"}},{"cell_type":"code","source":"# Save the Generator model's state dictionary to a file named 'generator.pth'\ntorch.save(netG.state_dict(), 'generator.pth')\n\n# Save the Discriminator model's state dictionary to a file named 'discriminator.pth'\ntorch.save(netD.state_dict(), 'discriminator.pth')","metadata":{"id":"IixqmS-kj8s-","execution":{"iopub.status.busy":"2024-10-16T05:08:07.002892Z","iopub.execute_input":"2024-10-16T05:08:07.003377Z","iopub.status.idle":"2024-10-16T05:08:07.042200Z","shell.execute_reply.started":"2024-10-16T05:08:07.003331Z","shell.execute_reply":"2024-10-16T05:08:07.041134Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Generating AI Video\n\nAs we discussed, our approach to test our model on unseen data is comparable to the example where our training data involves dogs fetching balls and cats chasing mice. Therefore, our test prompt could involve scenarios like a cat fetching a ball or a dog chasing a mouse.\nIn our specific case, the motion where the circle moves up and then to the right is not present in our training data, so the model is unfamiliar with this specific motion. However, it has been trained on other motions. We can use this motion as a prompt to test our trained model and observe its performance.","metadata":{"id":"dNlKX9P0j8s-"}},{"cell_type":"code","source":"# Inference function to generate a video based on a given text prompt\ndef generate_video(text_prompt, num_frames=10):\n    # Create a directory for the generated video frames based on the text prompt\n    os.makedirs(f'/kaggle/working/generated_video_{text_prompt.replace(\" \", \"_\")}', exist_ok=True)\n\n    # Encode the text prompt into a text embedding tensor\n    text_embed = text_embedding(encode_text(text_prompt).to(device)).mean(dim=0).unsqueeze(0)\n\n    # Generate frames for the video\n    for frame_num in range(num_frames):\n        # Generate random noise\n        noise = torch.randn(1, 100).to(device)\n\n        # Generate a fake frame using the Generator network\n        with torch.no_grad():\n            fake_frame = netG(noise, text_embed)\n\n        # Save the generated fake frame as an image file\n        save_image(fake_frame, f'/kaggle/working/generated_video_{text_prompt.replace(\" \", \"_\")}/frame_{frame_num}.png')\n\n# usage of the generate_video function with a specific text prompt\ngenerate_video('circle moving up-right')","metadata":{"id":"RALjTc8Tj8s-","execution":{"iopub.status.busy":"2024-10-16T05:08:07.043579Z","iopub.execute_input":"2024-10-16T05:08:07.044547Z","iopub.status.idle":"2024-10-16T05:08:07.143456Z","shell.execute_reply.started":"2024-10-16T05:08:07.044504Z","shell.execute_reply":"2024-10-16T05:08:07.142656Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"When we run the above code, it will generate a directory containing all the frames of our generated video. We need to use a bit of code to merge all these frames into a single short video.","metadata":{"id":"7iymU6CVj8s-"}},{"cell_type":"code","source":"# Define the path to your folder containing the PNG frames\nfolder_path = '/kaggle/working/generated_video_circle_moving_up-right'\n\n# Get the list of all PNG files in the folder\nimage_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n\n# Sort the images by name (assuming they are numbered sequentially)\nimage_files.sort()\n\n# Create a list to store the frames\nframes = []\n\n# Read each image and append it to the frames list\nfor image_file in image_files:\n  image_path = os.path.join(folder_path, image_file)\n  frame = cv2.imread(image_path)\n  frames.append(frame)\n\n# Convert the frames list to a numpy array for easier processing\nframes = np.array(frames)\n\n# Define the frame rate (frames per second)\nfps = 10\n\n# Create a video writer object\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter('generated_video.avi', fourcc, fps, (frames[0].shape[1], frames[0].shape[0]))\n\n# Write each frame to the video\nfor frame in frames:\n  out.write(frame)\n\n# Release the video writer\nout.release()","metadata":{"id":"Gl9z6qoNj8s-","execution":{"iopub.status.busy":"2024-10-16T05:08:07.146450Z","iopub.execute_input":"2024-10-16T05:08:07.146919Z","iopub.status.idle":"2024-10-16T05:08:07.328667Z","shell.execute_reply.started":"2024-10-16T05:08:07.146868Z","shell.execute_reply":"2024-10-16T05:08:07.327968Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Make sure the folder path points to where your newly generated video exists. After running this code, your AI video will have been successfully created. Let's see what it looks like.","metadata":{"id":"Sqy1GxSRj8s-"}},{"cell_type":"markdown","source":"## What's Missing?\nI've tested various aspects of this architecture, and found that the training data is the key. By including more motions and shapes in the dataset, you can increase variability and improve the model's performance. Since the data is generated through code, generating more varied data won't take much time; instead, you can focus on refining the logic.\nFurthermore, the GAN architecture discussed in this blog is relatively straightforward. You can make it more complex by integrating advanced techniques or using a language model embedding (LLM) instead of a basic neural network embedding. Additionally, tuning parameters such as embedding size and others can significantly impact the model's effectiveness.","metadata":{"id":"VRLTU_onj8s-"}}]}